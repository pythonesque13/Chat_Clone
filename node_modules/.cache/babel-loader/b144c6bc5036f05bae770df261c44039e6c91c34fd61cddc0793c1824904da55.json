{"ast":null,"code":"\"use strict\";\n\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.VectorStoreFilesPage = exports.FileBatches = void 0;\nconst resource_1 = require(\"../../../resource.js\");\nconst core_1 = require(\"../../../core.js\");\nconst core_2 = require(\"../../../core.js\");\nconst Util_1 = require(\"../../../lib/Util.js\");\nconst files_1 = require(\"./files.js\");\nObject.defineProperty(exports, \"VectorStoreFilesPage\", {\n  enumerable: true,\n  get: function () {\n    return files_1.VectorStoreFilesPage;\n  }\n});\nclass FileBatches extends resource_1.APIResource {\n  /**\n   * Create a vector store file batch.\n   */\n  create(vectorStoreId, body, options) {\n    return this._client.post(`/vector_stores/${vectorStoreId}/file_batches`, {\n      body,\n      ...options,\n      headers: {\n        'OpenAI-Beta': 'assistants=v2',\n        ...options?.headers\n      }\n    });\n  }\n  /**\n   * Retrieves a vector store file batch.\n   */\n  retrieve(vectorStoreId, batchId, options) {\n    return this._client.get(`/vector_stores/${vectorStoreId}/file_batches/${batchId}`, {\n      ...options,\n      headers: {\n        'OpenAI-Beta': 'assistants=v2',\n        ...options?.headers\n      }\n    });\n  }\n  /**\n   * Cancel a vector store file batch. This attempts to cancel the processing of\n   * files in this batch as soon as possible.\n   */\n  cancel(vectorStoreId, batchId, options) {\n    return this._client.post(`/vector_stores/${vectorStoreId}/file_batches/${batchId}/cancel`, {\n      ...options,\n      headers: {\n        'OpenAI-Beta': 'assistants=v2',\n        ...options?.headers\n      }\n    });\n  }\n  /**\n   * Create a vector store batch and poll until all files have been processed.\n   */\n  async createAndPoll(vectorStoreId, body, options) {\n    const batch = await this.create(vectorStoreId, body);\n    return await this.poll(vectorStoreId, batch.id, options);\n  }\n  listFiles(vectorStoreId, batchId) {\n    let query = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    let options = arguments.length > 3 ? arguments[3] : undefined;\n    if ((0, core_1.isRequestOptions)(query)) {\n      return this.listFiles(vectorStoreId, batchId, {}, query);\n    }\n    return this._client.getAPIList(`/vector_stores/${vectorStoreId}/file_batches/${batchId}/files`, files_1.VectorStoreFilesPage, {\n      query,\n      ...options,\n      headers: {\n        'OpenAI-Beta': 'assistants=v2',\n        ...options?.headers\n      }\n    });\n  }\n  /**\n   * Wait for the given file batch to be processed.\n   *\n   * Note: this will return even if one of the files failed to process, you need to\n   * check batch.file_counts.failed_count to handle this case.\n   */\n  async poll(vectorStoreId, batchId, options) {\n    const headers = {\n      ...options?.headers,\n      'X-Stainless-Poll-Helper': 'true'\n    };\n    if (options?.pollIntervalMs) {\n      headers['X-Stainless-Custom-Poll-Interval'] = options.pollIntervalMs.toString();\n    }\n    while (true) {\n      const {\n        data: batch,\n        response\n      } = await this.retrieve(vectorStoreId, batchId, {\n        ...options,\n        headers\n      }).withResponse();\n      switch (batch.status) {\n        case 'in_progress':\n          let sleepInterval = 5000;\n          if (options?.pollIntervalMs) {\n            sleepInterval = options.pollIntervalMs;\n          } else {\n            const headerInterval = response.headers.get('openai-poll-after-ms');\n            if (headerInterval) {\n              const headerIntervalMs = parseInt(headerInterval);\n              if (!isNaN(headerIntervalMs)) {\n                sleepInterval = headerIntervalMs;\n              }\n            }\n          }\n          await (0, core_2.sleep)(sleepInterval);\n          break;\n        case 'failed':\n        case 'cancelled':\n        case 'completed':\n          return batch;\n      }\n    }\n  }\n  /**\n   * Uploads the given files concurrently and then creates a vector store file batch.\n   *\n   * The concurrency limit is configurable using the `maxConcurrency` parameter.\n   */\n  async uploadAndPoll(vectorStoreId, _ref, options) {\n    let {\n      files,\n      fileIds = []\n    } = _ref;\n    if (files == null || files.length == 0) {\n      throw new Error(`No \\`files\\` provided to process. If you've already uploaded files you should use \\`.createAndPoll()\\` instead`);\n    }\n    const configuredConcurrency = options?.maxConcurrency ?? 5;\n    // We cap the number of workers at the number of files (so we don't start any unnecessary workers)\n    const concurrencyLimit = Math.min(configuredConcurrency, files.length);\n    const client = this._client;\n    const fileIterator = files.values();\n    const allFileIds = [...fileIds];\n    // This code is based on this design. The libraries don't accommodate our environment limits.\n    // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\n    async function processFiles(iterator) {\n      for (let item of iterator) {\n        const fileObj = await client.files.create({\n          file: item,\n          purpose: 'assistants'\n        }, options);\n        allFileIds.push(fileObj.id);\n      }\n    }\n    // Start workers to process results\n    const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\n    // Wait for all processing to complete.\n    await (0, Util_1.allSettledWithThrow)(workers);\n    return await this.createAndPoll(vectorStoreId, {\n      file_ids: allFileIds\n    });\n  }\n}\nexports.FileBatches = FileBatches;\n(function (FileBatches) {})(FileBatches = exports.FileBatches || (exports.FileBatches = {}));","map":{"version":3,"names":["resource_1","require","core_1","core_2","Util_1","files_1","Object","defineProperty","exports","enumerable","get","VectorStoreFilesPage","FileBatches","APIResource","create","vectorStoreId","body","options","_client","post","headers","retrieve","batchId","cancel","createAndPoll","batch","poll","id","listFiles","query","arguments","length","undefined","isRequestOptions","getAPIList","pollIntervalMs","toString","data","response","withResponse","status","sleepInterval","headerInterval","headerIntervalMs","parseInt","isNaN","sleep","uploadAndPoll","_ref","files","fileIds","Error","configuredConcurrency","maxConcurrency","concurrencyLimit","Math","min","client","fileIterator","values","allFileIds","processFiles","iterator","item","fileObj","file","purpose","push","workers","Array","fill","map","allSettledWithThrow","file_ids"],"sources":["/home/martin/Documents/codes/Prog_Web/Project_React/clone/node_modules/openai/src/resources/beta/vector-stores/file-batches.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from '../../../resource';\nimport { isRequestOptions } from '../../../core';\nimport { sleep } from '../../../core';\nimport { Uploadable } from '../../../core';\nimport { allSettledWithThrow } from '../../../lib/Util';\nimport * as Core from '../../../core';\nimport * as FileBatchesAPI from './file-batches';\nimport * as FilesAPI from './files';\nimport { VectorStoreFilesPage } from './files';\nimport { type CursorPageParams } from '../../../pagination';\n\nexport class FileBatches extends APIResource {\n  /**\n   * Create a vector store file batch.\n   */\n  create(\n    vectorStoreId: string,\n    body: FileBatchCreateParams,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<VectorStoreFileBatch> {\n    return this._client.post(`/vector_stores/${vectorStoreId}/file_batches`, {\n      body,\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n    });\n  }\n\n  /**\n   * Retrieves a vector store file batch.\n   */\n  retrieve(\n    vectorStoreId: string,\n    batchId: string,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<VectorStoreFileBatch> {\n    return this._client.get(`/vector_stores/${vectorStoreId}/file_batches/${batchId}`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n    });\n  }\n\n  /**\n   * Cancel a vector store file batch. This attempts to cancel the processing of\n   * files in this batch as soon as possible.\n   */\n  cancel(\n    vectorStoreId: string,\n    batchId: string,\n    options?: Core.RequestOptions,\n  ): Core.APIPromise<VectorStoreFileBatch> {\n    return this._client.post(`/vector_stores/${vectorStoreId}/file_batches/${batchId}/cancel`, {\n      ...options,\n      headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n    });\n  }\n\n  /**\n   * Create a vector store batch and poll until all files have been processed.\n   */\n  async createAndPoll(\n    vectorStoreId: string,\n    body: FileBatchCreateParams,\n    options?: Core.RequestOptions & { pollIntervalMs?: number },\n  ): Promise<VectorStoreFileBatch> {\n    const batch = await this.create(vectorStoreId, body);\n    return await this.poll(vectorStoreId, batch.id, options);\n  }\n\n  /**\n   * Returns a list of vector store files in a batch.\n   */\n  listFiles(\n    vectorStoreId: string,\n    batchId: string,\n    query?: FileBatchListFilesParams,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<VectorStoreFilesPage, FilesAPI.VectorStoreFile>;\n  listFiles(\n    vectorStoreId: string,\n    batchId: string,\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<VectorStoreFilesPage, FilesAPI.VectorStoreFile>;\n  listFiles(\n    vectorStoreId: string,\n    batchId: string,\n    query: FileBatchListFilesParams | Core.RequestOptions = {},\n    options?: Core.RequestOptions,\n  ): Core.PagePromise<VectorStoreFilesPage, FilesAPI.VectorStoreFile> {\n    if (isRequestOptions(query)) {\n      return this.listFiles(vectorStoreId, batchId, {}, query);\n    }\n    return this._client.getAPIList(\n      `/vector_stores/${vectorStoreId}/file_batches/${batchId}/files`,\n      VectorStoreFilesPage,\n      { query, ...options, headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers } },\n    );\n  }\n\n  /**\n   * Wait for the given file batch to be processed.\n   *\n   * Note: this will return even if one of the files failed to process, you need to\n   * check batch.file_counts.failed_count to handle this case.\n   */\n  async poll(\n    vectorStoreId: string,\n    batchId: string,\n    options?: Core.RequestOptions & { pollIntervalMs?: number },\n  ): Promise<VectorStoreFileBatch> {\n    const headers: { [key: string]: string } = { ...options?.headers, 'X-Stainless-Poll-Helper': 'true' };\n    if (options?.pollIntervalMs) {\n      headers['X-Stainless-Custom-Poll-Interval'] = options.pollIntervalMs.toString();\n    }\n\n    while (true) {\n      const { data: batch, response } = await this.retrieve(vectorStoreId, batchId, {\n        ...options,\n        headers,\n      }).withResponse();\n\n      switch (batch.status) {\n        case 'in_progress':\n          let sleepInterval = 5000;\n\n          if (options?.pollIntervalMs) {\n            sleepInterval = options.pollIntervalMs;\n          } else {\n            const headerInterval = response.headers.get('openai-poll-after-ms');\n            if (headerInterval) {\n              const headerIntervalMs = parseInt(headerInterval);\n              if (!isNaN(headerIntervalMs)) {\n                sleepInterval = headerIntervalMs;\n              }\n            }\n          }\n          await sleep(sleepInterval);\n          break;\n        case 'failed':\n        case 'cancelled':\n        case 'completed':\n          return batch;\n      }\n    }\n  }\n\n  /**\n   * Uploads the given files concurrently and then creates a vector store file batch.\n   *\n   * The concurrency limit is configurable using the `maxConcurrency` parameter.\n   */\n  async uploadAndPoll(\n    vectorStoreId: string,\n    { files, fileIds = [] }: { files: Uploadable[]; fileIds?: string[] },\n    options?: Core.RequestOptions & { pollIntervalMs?: number; maxConcurrency?: number },\n  ): Promise<VectorStoreFileBatch> {\n    if (files == null || files.length == 0) {\n      throw new Error(\n        `No \\`files\\` provided to process. If you've already uploaded files you should use \\`.createAndPoll()\\` instead`,\n      );\n    }\n\n    const configuredConcurrency = options?.maxConcurrency ?? 5;\n\n    // We cap the number of workers at the number of files (so we don't start any unnecessary workers)\n    const concurrencyLimit = Math.min(configuredConcurrency, files.length);\n\n    const client = this._client;\n    const fileIterator = files.values();\n    const allFileIds: string[] = [...fileIds];\n\n    // This code is based on this design. The libraries don't accommodate our environment limits.\n    // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\n    async function processFiles(iterator: IterableIterator<Uploadable>) {\n      for (let item of iterator) {\n        const fileObj = await client.files.create({ file: item, purpose: 'assistants' }, options);\n        allFileIds.push(fileObj.id);\n      }\n    }\n\n    // Start workers to process results\n    const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\n\n    // Wait for all processing to complete.\n    await allSettledWithThrow(workers);\n\n    return await this.createAndPoll(vectorStoreId, {\n      file_ids: allFileIds,\n    });\n  }\n}\n\n/**\n * A batch of files attached to a vector store.\n */\nexport interface VectorStoreFileBatch {\n  /**\n   * The identifier, which can be referenced in API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the vector store files batch was\n   * created.\n   */\n  created_at: number;\n\n  file_counts: VectorStoreFileBatch.FileCounts;\n\n  /**\n   * The object type, which is always `vector_store.file_batch`.\n   */\n  object: 'vector_store.files_batch';\n\n  /**\n   * The status of the vector store files batch, which can be either `in_progress`,\n   * `completed`, `cancelled` or `failed`.\n   */\n  status: 'in_progress' | 'completed' | 'cancelled' | 'failed';\n\n  /**\n   * The ID of the\n   * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object)\n   * that the [File](https://platform.openai.com/docs/api-reference/files) is\n   * attached to.\n   */\n  vector_store_id: string;\n}\n\nexport namespace VectorStoreFileBatch {\n  export interface FileCounts {\n    /**\n     * The number of files that where cancelled.\n     */\n    cancelled: number;\n\n    /**\n     * The number of files that have been processed.\n     */\n    completed: number;\n\n    /**\n     * The number of files that have failed to process.\n     */\n    failed: number;\n\n    /**\n     * The number of files that are currently being processed.\n     */\n    in_progress: number;\n\n    /**\n     * The total number of files.\n     */\n    total: number;\n  }\n}\n\nexport interface FileBatchCreateParams {\n  /**\n   * A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that\n   * the vector store should use. Useful for tools like `file_search` that can access\n   * files.\n   */\n  file_ids: Array<string>;\n\n  /**\n   * The chunking strategy used to chunk the file(s). If not set, will use the `auto`\n   * strategy.\n   */\n  chunking_strategy?:\n    | FileBatchCreateParams.AutoChunkingStrategyRequestParam\n    | FileBatchCreateParams.StaticChunkingStrategyRequestParam;\n}\n\nexport namespace FileBatchCreateParams {\n  /**\n   * The default strategy. This strategy currently uses a `max_chunk_size_tokens` of\n   * `800` and `chunk_overlap_tokens` of `400`.\n   */\n  export interface AutoChunkingStrategyRequestParam {\n    /**\n     * Always `auto`.\n     */\n    type: 'auto';\n  }\n\n  export interface StaticChunkingStrategyRequestParam {\n    static: StaticChunkingStrategyRequestParam.Static;\n\n    /**\n     * Always `static`.\n     */\n    type: 'static';\n  }\n\n  export namespace StaticChunkingStrategyRequestParam {\n    export interface Static {\n      /**\n       * The number of tokens that overlap between chunks. The default value is `400`.\n       *\n       * Note that the overlap must not exceed half of `max_chunk_size_tokens`.\n       */\n      chunk_overlap_tokens: number;\n\n      /**\n       * The maximum number of tokens in each chunk. The default value is `800`. The\n       * minimum value is `100` and the maximum value is `4096`.\n       */\n      max_chunk_size_tokens: number;\n    }\n  }\n}\n\nexport interface FileBatchListFilesParams extends CursorPageParams {\n  /**\n   * A cursor for use in pagination. `before` is an object ID that defines your place\n   * in the list. For instance, if you make a list request and receive 100 objects,\n   * ending with obj_foo, your subsequent call can include before=obj_foo in order to\n   * fetch the previous page of the list.\n   */\n  before?: string;\n\n  /**\n   * Filter by file status. One of `in_progress`, `completed`, `failed`, `cancelled`.\n   */\n  filter?: 'in_progress' | 'completed' | 'failed' | 'cancelled';\n\n  /**\n   * Sort order by the `created_at` timestamp of the objects. `asc` for ascending\n   * order and `desc` for descending order.\n   */\n  order?: 'asc' | 'desc';\n}\n\nexport namespace FileBatches {\n  export import VectorStoreFileBatch = FileBatchesAPI.VectorStoreFileBatch;\n  export import FileBatchCreateParams = FileBatchesAPI.FileBatchCreateParams;\n  export import FileBatchListFilesParams = FileBatchesAPI.FileBatchListFilesParams;\n}\n\nexport { VectorStoreFilesPage };\n"],"mappings":";;AAAA;;;;;AAEA,MAAAA,UAAA,GAAAC,OAAA;AACA,MAAAC,MAAA,GAAAD,OAAA;AACA,MAAAE,MAAA,GAAAF,OAAA;AAEA,MAAAG,MAAA,GAAAH,OAAA;AAIA,MAAAI,OAAA,GAAAJ,OAAA;AA4USK,MAAA,CAAAC,cAAA,CAAAC,OAAA;EAAAC,UAAA;EAAAC,GAAA,WAAAA,CAAA;IAAA,OA5UAL,OAAA,CAAAM,oBAAoB;EAAA;AAAA;AAG7B,MAAaC,WAAY,SAAQZ,UAAA,CAAAa,WAAW;EAC1C;;;EAGAC,MAAMA,CACJC,aAAqB,EACrBC,IAA2B,EAC3BC,OAA6B;IAE7B,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,kBAAkBJ,aAAa,eAAe,EAAE;MACvEC,IAAI;MACJ,GAAGC,OAAO;MACVG,OAAO,EAAE;QAAE,aAAa,EAAE,eAAe;QAAE,GAAGH,OAAO,EAAEG;MAAO;KAC/D,CAAC;EACJ;EAEA;;;EAGAC,QAAQA,CACNN,aAAqB,EACrBO,OAAe,EACfL,OAA6B;IAE7B,OAAO,IAAI,CAACC,OAAO,CAACR,GAAG,CAAC,kBAAkBK,aAAa,iBAAiBO,OAAO,EAAE,EAAE;MACjF,GAAGL,OAAO;MACVG,OAAO,EAAE;QAAE,aAAa,EAAE,eAAe;QAAE,GAAGH,OAAO,EAAEG;MAAO;KAC/D,CAAC;EACJ;EAEA;;;;EAIAG,MAAMA,CACJR,aAAqB,EACrBO,OAAe,EACfL,OAA6B;IAE7B,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,kBAAkBJ,aAAa,iBAAiBO,OAAO,SAAS,EAAE;MACzF,GAAGL,OAAO;MACVG,OAAO,EAAE;QAAE,aAAa,EAAE,eAAe;QAAE,GAAGH,OAAO,EAAEG;MAAO;KAC/D,CAAC;EACJ;EAEA;;;EAGA,MAAMI,aAAaA,CACjBT,aAAqB,EACrBC,IAA2B,EAC3BC,OAA2D;IAE3D,MAAMQ,KAAK,GAAG,MAAM,IAAI,CAACX,MAAM,CAACC,aAAa,EAAEC,IAAI,CAAC;IACpD,OAAO,MAAM,IAAI,CAACU,IAAI,CAACX,aAAa,EAAEU,KAAK,CAACE,EAAE,EAAEV,OAAO,CAAC;EAC1D;EAgBAW,SAASA,CACPb,aAAqB,EACrBO,OAAe,EAEc;IAAA,IAD7BO,KAAA,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAwD,EAAE;IAAA,IAC1Db,OAA6B,GAAAa,SAAA,CAAAC,MAAA,OAAAD,SAAA,MAAAE,SAAA;IAE7B,IAAI,IAAA9B,MAAA,CAAA+B,gBAAgB,EAACJ,KAAK,CAAC,EAAE;MAC3B,OAAO,IAAI,CAACD,SAAS,CAACb,aAAa,EAAEO,OAAO,EAAE,EAAE,EAAEO,KAAK,CAAC;;IAE1D,OAAO,IAAI,CAACX,OAAO,CAACgB,UAAU,CAC5B,kBAAkBnB,aAAa,iBAAiBO,OAAO,QAAQ,EAC/DjB,OAAA,CAAAM,oBAAoB,EACpB;MAAEkB,KAAK;MAAE,GAAGZ,OAAO;MAAEG,OAAO,EAAE;QAAE,aAAa,EAAE,eAAe;QAAE,GAAGH,OAAO,EAAEG;MAAO;IAAE,CAAE,CACxF;EACH;EAEA;;;;;;EAMA,MAAMM,IAAIA,CACRX,aAAqB,EACrBO,OAAe,EACfL,OAA2D;IAE3D,MAAMG,OAAO,GAA8B;MAAE,GAAGH,OAAO,EAAEG,OAAO;MAAE,yBAAyB,EAAE;IAAM,CAAE;IACrG,IAAIH,OAAO,EAAEkB,cAAc,EAAE;MAC3Bf,OAAO,CAAC,kCAAkC,CAAC,GAAGH,OAAO,CAACkB,cAAc,CAACC,QAAQ,EAAE;;IAGjF,OAAO,IAAI,EAAE;MACX,MAAM;QAAEC,IAAI,EAAEZ,KAAK;QAAEa;MAAQ,CAAE,GAAG,MAAM,IAAI,CAACjB,QAAQ,CAACN,aAAa,EAAEO,OAAO,EAAE;QAC5E,GAAGL,OAAO;QACVG;OACD,CAAC,CAACmB,YAAY,EAAE;MAEjB,QAAQd,KAAK,CAACe,MAAM;QAClB,KAAK,aAAa;UAChB,IAAIC,aAAa,GAAG,IAAI;UAExB,IAAIxB,OAAO,EAAEkB,cAAc,EAAE;YAC3BM,aAAa,GAAGxB,OAAO,CAACkB,cAAc;WACvC,MAAM;YACL,MAAMO,cAAc,GAAGJ,QAAQ,CAAClB,OAAO,CAACV,GAAG,CAAC,sBAAsB,CAAC;YACnE,IAAIgC,cAAc,EAAE;cAClB,MAAMC,gBAAgB,GAAGC,QAAQ,CAACF,cAAc,CAAC;cACjD,IAAI,CAACG,KAAK,CAACF,gBAAgB,CAAC,EAAE;gBAC5BF,aAAa,GAAGE,gBAAgB;;;;UAItC,MAAM,IAAAxC,MAAA,CAAA2C,KAAK,EAACL,aAAa,CAAC;UAC1B;QACF,KAAK,QAAQ;QACb,KAAK,WAAW;QAChB,KAAK,WAAW;UACd,OAAOhB,KAAK;;;EAGpB;EAEA;;;;;EAKA,MAAMsB,aAAaA,CACjBhC,aAAqB,EAAAiC,IAAA,EAErB/B,OAAoF;IAAA,IADpF;MAAEgC,KAAK;MAAEC,OAAO,GAAG;IAAE,CAA+C,GAAAF,IAAA;IAGpE,IAAIC,KAAK,IAAI,IAAI,IAAIA,KAAK,CAAClB,MAAM,IAAI,CAAC,EAAE;MACtC,MAAM,IAAIoB,KAAK,CACb,gHAAgH,CACjH;;IAGH,MAAMC,qBAAqB,GAAGnC,OAAO,EAAEoC,cAAc,IAAI,CAAC;IAE1D;IACA,MAAMC,gBAAgB,GAAGC,IAAI,CAACC,GAAG,CAACJ,qBAAqB,EAAEH,KAAK,CAAClB,MAAM,CAAC;IAEtE,MAAM0B,MAAM,GAAG,IAAI,CAACvC,OAAO;IAC3B,MAAMwC,YAAY,GAAGT,KAAK,CAACU,MAAM,EAAE;IACnC,MAAMC,UAAU,GAAa,CAAC,GAAGV,OAAO,CAAC;IAEzC;IACA;IACA,eAAeW,YAAYA,CAACC,QAAsC;MAChE,KAAK,IAAIC,IAAI,IAAID,QAAQ,EAAE;QACzB,MAAME,OAAO,GAAG,MAAMP,MAAM,CAACR,KAAK,CAACnC,MAAM,CAAC;UAAEmD,IAAI,EAAEF,IAAI;UAAEG,OAAO,EAAE;QAAY,CAAE,EAAEjD,OAAO,CAAC;QACzF2C,UAAU,CAACO,IAAI,CAACH,OAAO,CAACrC,EAAE,CAAC;;IAE/B;IAEA;IACA,MAAMyC,OAAO,GAAGC,KAAK,CAACf,gBAAgB,CAAC,CAACgB,IAAI,CAACZ,YAAY,CAAC,CAACa,GAAG,CAACV,YAAY,CAAC;IAE5E;IACA,MAAM,IAAAzD,MAAA,CAAAoE,mBAAmB,EAACJ,OAAO,CAAC;IAElC,OAAO,MAAM,IAAI,CAAC5C,aAAa,CAACT,aAAa,EAAE;MAC7C0D,QAAQ,EAAEb;KACX,CAAC;EACJ;;AAjLFpD,OAAA,CAAAI,WAAA,GAAAA,WAAA;AAmUA,WAAiBA,WAAW,GAI5B,CAAC,EAJgBA,WAAW,GAAXJ,OAAA,CAAAI,WAAW,KAAXJ,OAAA,CAAAI,WAAW","ignoreList":[]},"metadata":{},"sourceType":"script","externalDependencies":[]}